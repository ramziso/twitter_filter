{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Spark on Colaboratory.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramziso/twitter_filter/blob/master/Copy_of_Copy_of_Spark_on_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvD4HBMi0ohY",
        "colab_type": "text"
      },
      "source": [
        "# Install Java, Spark, and Findspark\n",
        "This installs Apache Spark 2.2.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUhBhrGmyAvs",
        "colab_type": "code",
        "outputId": "12ae4b9c-5dd5-48f1-9775-6ba73cc363ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.3.4/spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!ls -l\n",
        "!tar xf spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!wget -q https://repo1.maven.org/maven2/com/redislabs/spark-redis/2.3.0/spark-redis-2.3.0-jar-with-dependencies.jar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 221796\n",
            "drwxr-xr-x 1 root root      4096 Oct 22 15:18 sample_data\n",
            "-rw-r--r-- 1 root root 227110658 Aug 26 09:00 spark-2.3.4-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Kjvk_h1AHl",
        "colab_type": "text"
      },
      "source": [
        "# Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xnb_ePUyQIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.4-bin-hadoop2.7\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--jars /content/spark-redis-2.3.0-jar-with-dependencies.jar pyspark-shell\"\n",
        "\n",
        "#--packages com.redislabs:spark-redis:2.3.0\n",
        "#os.environ['PYSPARK_SUBMIT_ARGS'] = \"--jars /content/spark-redis-2.3.0.jar pyspark-shell\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOQ6aeljB1IG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/spark-redis-2.3.0-jar-with-dependencies.jar /content/spark-2.3.4-bin-hadoop2.7/python/lib/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXf2_jrLAp2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content/spark-2.3.4-bin-hadoop2.7/jars\")\n",
        "sys.path.insert(0, \"/content/spark-2.3.4-bin-hadoop2.7/python/pyspark\")\n",
        "sys.path.insert(0, os.path.join('SPARK_HOME','/content/spark-redis-2.3.0-jar-with-dependencies.jar'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwU28K5f1H3P",
        "colab_type": "text"
      },
      "source": [
        "# Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgReRGl0y23D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.redis.host\", \"SG-ramzi-27118.servers.mongodirector.com\").config(\"spark.redis.port\", \"6379\").config(\"spark.redis.auth\", \"QLbG0tiG65nKq1JgaPmK3y48tlSzUGYc\").config('spark.jars.packages', 'com.redislabs:spark-redis:2.3.0').getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "tweet_schema = StructType([\n",
        "    StructField('lang', StringType(), True),\n",
        "    StructField('text', StringType(), True)\n",
        "    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82foqheODhXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8SdNwHHGvf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo \"spark.jars.packages               com.redislabs:spark-redis:2.3.0\" >> $SPARK_HOME/conf/spark-defaults.conf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpVC9roBHGHB",
        "colab_type": "code",
        "outputId": "d364f7af-1397-418a-ae14-7477510625d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cat $SPARK_HOME/conf/spark-defaults.conf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark.jars.packages               com.redislabs:spark-redis:2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8w_YRp1vtg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp spark-redis-2.3.0-jar-with-dependencies.jar /content/spark-2.3.4-bin-hadoop2.7/jars\n",
        "!ls -l /content/spark-2.3.4-bin-hadoop2.7/jars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV1UTcMrtzdF",
        "colab_type": "code",
        "outputId": "0c4ea31d-2dc9-47ef-942c-5818caa78a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "loadedDf = spark.read.format(\"org.apache.spark.sql.redis\").option(\"table\", \"twitter\").load()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6d43a7ef6b40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloadedDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.apache.spark.sql.redis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o75.load.\n: java.lang.ClassNotFoundException: Failed to find data source: org.apache.spark.sql.redis. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:639)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.redis.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$15.apply(DataSource.scala:622)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23$$anonfun$apply$15.apply(DataSource.scala:622)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:622)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$23.apply(DataSource.scala:622)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:622)\n\t... 13 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gEEm_rBClea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loadedDf.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ULPx4Y1LiR",
        "colab_type": "text"
      },
      "source": [
        "# Use Spark!\n",
        "That's all there is to it - you're ready to use Spark!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJp8ZI-VzYEz",
        "colab_type": "code",
        "outputId": "e384c848-fca7-43bd-cf29-c214bb3b68bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-2.3.4-bin-hadoop2.7/python/pyspark/sql/session.py:340: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blAOov3mHTfU",
        "colab_type": "code",
        "outputId": "57f0712f-f9d5-474c-9bdd-38db4cd04505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install redis"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting redis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/ae/28613a62eea0d53d3db3147f8715f90da07667e99baeedf1010eb400f8c0/redis-3.3.11-py2.py3-none-any.whl (66kB)\n",
            "\r\u001b[K     |█████                           | 10kB 25.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: redis\n",
            "Successfully installed redis-3.3.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZkw_gPEQvId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import redis\n",
        "\n",
        "REDIS_HOST = 'SG-ramzi-27118.servers.mongodirector.com'\n",
        "REDIS_PORT = 6379\n",
        "REDIS_LIST = 'ramzi'\n",
        "\n",
        "# Redis Configuration\n",
        "redis_password = \"QLbG0tiG65nKq1JgaPmK3y48tlSzUGYc\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBaPA8D8hRFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, password=redis_password, db=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcx3lrYjhldx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "class TweetStore:\n",
        "\n",
        "    # Redis Configuration\n",
        "    redis_host = \"SG-ramzi-27118.servers.mongodirector.com\"\n",
        "    redis_port = 6379\n",
        "    redis_password = \"QLbG0tiG65nKq1JgaPmK3y48tlSzUGYc\"\n",
        "    redis_db = 0\n",
        "    \n",
        "    # Tweet Configuration\n",
        "    redis_key = 'tweets'\n",
        "    num_tweets = 20\n",
        "\n",
        "    def __init__(self):\n",
        "        self.db = r = redis.Redis(\n",
        "            host=self.redis_host,\n",
        "            port=self.redis_port,\n",
        "            password=self.redis_password,\n",
        "            db=self.redis_db\n",
        "        )\n",
        "        self.trim_count = 0\n",
        "\n",
        "    def push(self, data):\n",
        "        self.db.lpush(self.redis_key, json.dumps(data))\n",
        "        self.trim_count += 1\n",
        "\n",
        "        # Periodically trim the list so it doesn't grow too large.\n",
        "        if self.trim_count > 100:\n",
        "            self.db.ltrim(self.redis_key, 0, self.num_tweets)\n",
        "            self.trim_count = 0\n",
        "\n",
        "    def tweets(self, limit=15):\n",
        "        tweets = []\n",
        "\n",
        "        for item in self.db.lrange(self.redis_key, 0, limit-1):\n",
        "            tweet_obj = json.loads(item)\n",
        "            tweets.append(tweet_obj)\n",
        "\n",
        "        return tweets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKEMnpaO1Kyv",
        "colab_type": "code",
        "outputId": "3c9501ab-000f-4d5e-ae92-f13f502bf623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r.set('foo','bar')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWbB3dWLVF0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "f2937a32-aebc-4466-f5e2-a9db0ae9dc6e"
      },
      "source": [
        "import json\n",
        "aa = r.lrange('twitter', 0, -1)\n",
        "type (aa)\n",
        "# Decode UTF-8 bytes to Unicode, and convert single quotes \n",
        "# to double quotes to make it valid JSON\n",
        "#my_json = aa[:10].decode('utf8').replace(\"'\", '\"')\n",
        "my_json = aa.decode('utf8').replace(\"'\", '\"')\n",
        "print(my_json)\n",
        "#print('- ' * 20)\n",
        "\n",
        "# Load the JSON to a Python list & dump it back out as formatted JSON\n",
        "data = json.loads(my_json)\n",
        "s = json.dumps(data, indent=4, sort_keys=True)\n",
        "print(s)\n",
        "\n",
        "#with open('/content/dump.json', 'w') as f:\n",
        "#        json.dump(aa, f)\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-9145dcea0f2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Decode UTF-8 bytes to Unicode, and convert single quotes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# to double quotes to make it valid JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmy_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#my_json = aa.decode('utf8').replace(\"'\", '\"')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'decode'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSWSv_IgHqP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install redis-dump-load"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptl3m2phGw1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "data = {\n",
        "    'foo': 'bar',\n",
        "    'ans': 42\n",
        "}\n",
        "r.execute_command('JSON.SET', 'object', '.', json.dumps(data))\n",
        "reply = json.loads(r.execute_command('JSON.GET', 'object'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FURLA5Y11M8J",
        "colab_type": "code",
        "outputId": "c1a4fac4-1cf7-403f-cc95-32462234b445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r.get('foo')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'bar'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}